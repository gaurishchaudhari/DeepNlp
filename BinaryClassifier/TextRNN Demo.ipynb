{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Classification (Binary) on Imdb Review Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import importlib\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from bunch import Bunch\n",
    "from sklearn.utils import shuffle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-11-10 12:06:10.433621] Loading data ...\n",
      "[2019-11-10 12:13:36.041301] Building vocab and loading glove embeddings ...\n",
      "[2019-11-10 12:13:48.744785] Creating X, y sequences ...\n",
      "[2019-11-10 12:13:54.511995] Padding X, y sequences ...\n"
     ]
    }
   ],
   "source": [
    "from helpers.dataset import Dataset\n",
    "\n",
    "glove_file = 'C:\\\\Workspace\\\\Git\\\\Data\\\\glove.6B\\\\glove.6B.100d.txt'\n",
    "data_root = 'C:\\\\Workspace\\\\Git\\\\Data\\\\aclImdb'\n",
    "categories = ['neg', 'pos']\n",
    "\n",
    "# Load dataset\n",
    "ds = Dataset(num_classes=2)\n",
    "print('[{}] Loading data ...'.format(datetime.now()))\n",
    "ds.load_dataset_from_dir(os.path.join(data_root, 'train'), os.path.join(data_root, 'test'), categories)\n",
    "\n",
    "print('[{}] Building vocab and loading glove embeddings ...'.format(datetime.now()))\n",
    "ds.build_vocab()\n",
    "ds.load_word_embeddings(glove_file)\n",
    "\n",
    "print('[{}] Creating X, y sequences ...'.format(datetime.now()))\n",
    "ds.compute_Xy()\n",
    "\n",
    "print('[{}] Padding X, y sequences ...'.format(datetime.now()))\n",
    "ds.pad_Xy(fixed_max_len=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Raw Training records: 25000 examples, 25000 labels \n",
      "Number of Raw Test records: 25000 examples, 25000 labels \n",
      "Target Names: ['neg', 'pos']\n",
      "Label distribution in Training Set\n",
      "[[    0 12500]\n",
      " [    1 12500]]\n",
      "Label distribution in Test Set\n",
      "[[    0 12500]\n",
      " [    1 12500]]\n",
      "Vocab Size:  53858\n",
      "shape(word_embeddings):  (46197, 100)\n",
      "Top 5 Words in word2index map ['$PAD$(0)', '$UNK$(1)', 'the(2)', 'of(3)', 'to(4)', 'and(5)']\n",
      "shape(ds.X_train) =  (25000, 30)  dtype(ds.X_train) =  int64\n",
      "shape(ds.y_train) =  (25000, 1)  dtype(ds.y_train) =  int32\n",
      "shape(ds.X_test) =  (25000, 30)  dtype(ds.X_test) =  int64\n",
      "shape(ds.y_test) =  (25000, 1)  dtype(ds.y_test) =  int32\n",
      "Top 5 training examples:\n",
      "[#1]\t [0]\n",
      "\t [4904  339 2920  339   11    2  174   65    3    2]\n",
      "\t silent night deadly night is the very last of the series and like part it s unrelated to the first three except by title and the fact that it s\n",
      "[#2]\t [1]\n",
      "\t [    2  1072 24523     7   174   595   295    14     7   494]\n",
      "\t the idea ia a very short film with a lot of information interesting entertaining and leaves the viewer wanting more the producer has produced a short film of excellent quality\n",
      "[#3]\t [0]\n",
      "\t [   8  262   30  926  105 1589    4  743   10   39]\n",
      "\t for me this movie just seemed to fall on its face the main problem for me was the casting of glover as a serial killer i don t know whether\n",
      "[#4]\t [1]\n",
      "\t [  12   30  223   10    7 3935  503    7  897  170]\n",
      "\t was this based on a comic book a video game a drawing by a year old\n",
      "[#5]\t [1]\n",
      "\t [ 6681    92  3288 29045     0     0     0     0     0     0]\n",
      "\t caution may contain spoilers\n"
     ]
    }
   ],
   "source": [
    "def print_stats(ds):\n",
    "\n",
    "    print('Number of Raw Training records: {} examples, {} labels '.format(\n",
    "        len(ds.raw_train.data), len(ds.raw_train.target)))\n",
    "    print('Number of Raw Test records: {} examples, {} labels '.format(\n",
    "        len(ds.raw_test.data), len(ds.raw_test.target)))\n",
    "    print('Target Names:', ds.raw_train.target_names)\n",
    "\n",
    "    print('Label distribution in Training Set')\n",
    "    print(np.asarray(np.unique(ds.raw_train.target, return_counts=True)).T)\n",
    "    print('Label distribution in Test Set')\n",
    "    print(np.asarray(np.unique(ds.raw_test.target, return_counts=True)).T)\n",
    "\n",
    "    print('Vocab Size: ', len(ds.vocab))\n",
    "    print('shape(word_embeddings): ', ds.word_embeddings.shape)\n",
    "    print('Top 5 Words in word2index map', ['{}({})'.format(w, i) for w, i in ds.word2index.items() if i <= 5])\n",
    "\n",
    "    print('shape(ds.X_train) = ', ds.X_train.shape, ' dtype(ds.X_train) = ', ds.X_train.dtype)\n",
    "    print('shape(ds.y_train) = ', ds.y_train.shape, ' dtype(ds.y_train) = ', ds.y_train.dtype)\n",
    "    print('shape(ds.X_test) = ', ds.X_test.shape, ' dtype(ds.X_test) = ', ds.X_test.dtype)\n",
    "    print('shape(ds.y_test) = ', ds.y_test.shape, ' dtype(ds.y_test) = ', ds.y_test.dtype)\n",
    "\n",
    "    print('Top 5 training examples:')\n",
    "    for i in range(5):\n",
    "        print('[#{}]\\t {}'.format(i + 1, ds.y_train[i]))\n",
    "        print('\\t', ds.X_train[i][:10])\n",
    "        print('\\t', ' '.join(ds.raw_train.data[i].split()[:30]))\n",
    "        \n",
    "print_stats(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape(X_train) =  (7000, 30)  dtype(X_train) =  int64\n",
      "shape(y_train) =  (7000, 1)  dtype(y_train) =  int32\n",
      "shape(X_val) =  (3000, 30)  dtype(X_val) =  int64\n",
      "shape(y_val) =  (3000, 1)  dtype(y_val) =  int32\n",
      "Label distribution in Training Set\n",
      "[[   0 3532]\n",
      " [   1 3468]]\n",
      "Label distribution in Validation Set\n",
      "[[   0 1483]\n",
      " [   1 1517]]\n"
     ]
    }
   ],
   "source": [
    "maxN = min(10000, len(ds.X_train)) \n",
    "split = int(0.7 * maxN)\n",
    "\n",
    "offset = 5000\n",
    "\n",
    "X_train = ds.X_train[offset:offset+split]\n",
    "y_train = ds.y_train[offset:offset+split]\n",
    "X_train_seqlen = ds.X_train_seqlen[offset:offset+split]\n",
    "\n",
    "X_val = ds.X_train[offset+split:offset+maxN]\n",
    "y_val = ds.y_train[offset+split:offset+maxN]\n",
    "X_val_seqlen = ds.X_train_seqlen[offset+split:offset+maxN]\n",
    "\n",
    "word_embeddings = ds.word_embeddings\n",
    "\n",
    "print('shape(X_train) = ', X_train.shape, ' dtype(X_train) = ', X_train.dtype)\n",
    "print('shape(y_train) = ', y_train.shape, ' dtype(y_train) = ', y_train.dtype)\n",
    "print('shape(X_val) = ', X_val.shape, ' dtype(X_val) = ', X_val.dtype)\n",
    "print('shape(y_val) = ', y_val.shape, ' dtype(y_val) = ', y_val.dtype)\n",
    "    \n",
    "print('Label distribution in Training Set')\n",
    "print(np.asarray(np.unique(y_train, return_counts=True)).T)\n",
    "print('Label distribution in Validation Set')\n",
    "print(np.asarray(np.unique(y_val, return_counts=True)).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Bunch()\n",
    "config.vocab_size, config.embed_size = word_embeddings.shape\n",
    "config.batch_size = 64\n",
    "config.lr = 0.001\n",
    "config.hidden_size = 50\n",
    "config.num_lstm_layers = 2\n",
    "config.dropout_p = 0.5\n",
    "config.isBidirectional = True\n",
    "config.output_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.utils import mini_batches\n",
    "from helpers.TextRNN import TextRNN\n",
    "from helpers.evaluate import evaluate_model\n",
    "import torch.optim as optim\n",
    "\n",
    "model = TextRNN(config, torch.from_numpy(word_embeddings).type(torch.FloatTensor))\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=config.lr)\n",
    "loss_op = nn.BCELoss()  # binary cross entropy loss\n",
    "\n",
    "model.set_optimizer(optimizer)\n",
    "model.set_loss_op(loss_op)\n",
    "\n",
    "epoch_losses = []\n",
    "epoch_metrics = []\n",
    "train_losses = []\n",
    "val_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 0.68492922, Training: 62.600% (=4382/7000) L=0.65252, Validation: 63.900% (=1917/3000), L=0.65190\n",
      "Updated Learning Rate =  {0.0005}\n",
      "Epoch: 2, Loss: 0.63917352, Training: 66.643% (=4665/7000) L=0.61315, Validation: 66.900% (=2007/3000), L=0.61534\n",
      "Epoch: 3, Loss: 0.60677087, Training: 68.086% (=4766/7000) L=0.58971, Validation: 67.867% (=2036/3000), L=0.59589\n",
      "Epoch: 4, Loss: 0.58206842, Training: 70.714% (=4950/7000) L=0.56565, Validation: 69.533% (=2086/3000), L=0.57718\n",
      "Updated Learning Rate =  {0.00025}\n",
      "Epoch: 5, Loss: 0.56787539, Training: 71.014% (=4971/7000) L=0.55104, Validation: 70.333% (=2110/3000), L=0.56798\n",
      "Epoch: 6, Loss: 0.55466367, Training: 71.543% (=5008/7000) L=0.54209, Validation: 70.633% (=2119/3000), L=0.56553\n",
      "Epoch: 7, Loss: 0.54951194, Training: 72.386% (=5067/7000) L=0.53412, Validation: 70.400% (=2112/3000), L=0.56397\n",
      "Updated Learning Rate =  {0.000125}\n",
      "Epoch: 8, Loss: 0.54105540, Training: 72.943% (=5106/7000) L=0.52997, Validation: 70.733% (=2122/3000), L=0.56218\n",
      "Epoch: 9, Loss: 0.53707586, Training: 73.243% (=5127/7000) L=0.52570, Validation: 70.700% (=2121/3000), L=0.56152\n"
     ]
    }
   ],
   "source": [
    "def reduce_lr(optimizer):\n",
    "    for g in optimizer.param_groups:\n",
    "        g['lr'] = g['lr'] / 2\n",
    "\n",
    "for epoch in range(10):\n",
    "        \n",
    "    #X_train, y_train, X_train_seqlen = shuffle(X_train, y_train, X_train_seqlen, random_state=17)\n",
    "    epoch_loss = model.run_epoch(X_train, y_train, X_train_seqlen)\n",
    "    \n",
    "    accuracy1, correct1, total1, loss1 = evaluate_model(model, X_train, y_train, X_train_seqlen, config.batch_size)\n",
    "    accuracy2, correct2, total2, loss2 = evaluate_model(model, X_val, y_val, X_val_seqlen, config.batch_size)\n",
    "    \n",
    "    epoch_losses.append(epoch_loss)\n",
    "    train_losses.append(loss1)\n",
    "    val_losses.append(loss2)\n",
    "    epoch_metrics.append('Training: {:.3f}% (={}/{}) L={:.5f}, Validation: {:.3f}% (={}/{}), L={:.5f}'.format(\n",
    "        accuracy1, correct1, total1, loss1, accuracy2, correct2, total2, loss2))\n",
    "    print('Epoch: {}, Loss: {:.8f}, {}'.format(epoch+1, epoch_losses[-1], epoch_metrics[-1]))\n",
    "    \n",
    "    if epoch % 3 == 0:\n",
    "        reduce_lr(model.optimizer)\n",
    "        print('Updated Learning Rate = ', set(g['lr'] for g in model.optimizer.param_groups))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses)\n",
    "plt.plot(val_losses)\n",
    "plt.title('TextRNN')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train loss', 'validation loss'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy, correct, total, loss = evaluate_model(model, ds.X_test, ds.y_test, ds.X_test_seqlen, config.batch_size)\n",
    "metric = '{:.3f}% (={}/{}) L={:.5f}'.format(accuracy, correct, total, loss)\n",
    "print('Test Set Metric: ', metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.evaluate import predict_instance\n",
    "\n",
    "for text in ['this is a good movie', 'this is worst movie i have seen']:\n",
    "    lbl, score = predict_instance(model, ds, text)\n",
    "    print('LABEL = {}, SCORE={:.4f}, INPUT = {}'.format(lbl, score, text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Understand Operations\n",
    "\n",
    "word_emb = torch.tensor([\n",
    "    [0,0,0,0,0],\n",
    "    [1,1,1,1,1],\n",
    "    [2,2,2,2,2],\n",
    "    [3,3,3,3,3],\n",
    "    [4,4,4,4,4],\n",
    "    [5,5,5,5,5],\n",
    "    [6,6,6,6,6],\n",
    "    [7,7,7,7,7],\n",
    "    [8,8,8,8,8]]).type(torch.FloatTensor)\n",
    "\n",
    "embeddings = nn.Embedding(num_embeddings=9, embedding_dim=5, padding_idx=0)   # V * d\n",
    "embeddings.weight = nn.Parameter(word_emb, requires_grad=False)\n",
    "\n",
    "lstm = nn.LSTM(input_size=5, hidden_size=7, num_layers=2, batch_first=True, bidirectional=True)\n",
    "\n",
    "X = torch.tensor([[2,3,4],[6,7,0]]).type(torch.LongTensor)\n",
    "\n",
    "print('X = ')\n",
    "print(X)\n",
    "eX = embeddings(X)\n",
    "print('eX = ')\n",
    "print(eX)\n",
    "l_o, (h_n, c_n) = lstm(eX)\n",
    "print('h_n = ')\n",
    "print(h_n)\n",
    "fi = torch.cat([h_n[i, :, :] for i in range(h_n.shape[0])], dim=1)\n",
    "print('fi = ')\n",
    "print(fi)\n",
    "fv = h_n.view(-1, h_n.size()[0] * h_n.size()[2])\n",
    "print('fv = ')\n",
    "print(fv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
